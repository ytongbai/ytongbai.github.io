<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script> -->
  <!-- <script type="text/javascript" src="js/hidebib.js"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Yutong Bai</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Yutong Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="images/JHU_icon.jpg">
  <style>
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-size: 16px;
      font-weight: 400;
      color: #282828;
    }

    papertitle {
      font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif !important;
      font-size: 16px !important;
      font-weight: bold !important;
      color: #337ab7 !important;
    }

    .pub-filter-menu {
      font-size: 16px;
      margin-bottom: 20px;
      color: #282828;
      font-family: inherit;
      margin-left: 0;
      padding-left: 0;
    }

    heading {
      display: block;
      margin-bottom: 6px;
      margin-left: 0;
      padding-left: 0;
    }
  </style>
</head>

<body>
  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1%;width:20%;vertical-align:top">
              <a href="images/yutong.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yutong.png" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:80%;vertical-align:middle">
              <p style="text-align:left">
                <name>Yutong Bai</name>
              </p>
                <p>Yutong is currently a Postdoc Researcher at UC Berkeley (<a href="https://bair.berkeley.edu/">BAIR</a>), advised by Prof. <a href="http://people.eecs.berkeley.edu/~efros/"> Alexei (Alyosha) Efros</a>,  Prof.<a href="https://people.eecs.berkeley.edu/~malik/"> Jitendra Malik</a> and Prof. <a href="https://people.eecs.berkeley.edu/~trevor/"> Trevor Darrell</a>. 
                
                Prior to that, she obtained CS PhD degree at Johns Hopkins University advised by Prof.<a href="https://www.cs.jhu.edu/~ayuille/"> Alan Yuille</a>.</p>

                <p>She used to intern at Meta AI (FAIR Labs) and Google Brain, and is selected as 2023 <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023">Apple Scholar</a> and <a href="https://risingstars-eecs.mit.edu/participants/yutong-bai-2/">MIT EECS Rising Star</a>.</p>

              <p style="text-align:left">
                <a href="mailto:ytongbai@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/YutongBAI1002">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ytongbai">Github</a> &nbsp
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <div style="font-size:20px;font-weight:bold;display:inline;">News</div>
              <div class="news-toggle-menu" style="display:inline; margin-left: 10px;">
                (
                  <span id="news-toggle" onclick="toggleNews()" style="color: #394FC1; cursor: pointer; font-weight: 400;">▼ Collapse</span>
                )
              </div>
            </td>
          </tr>
        </tbody></table>

        <table id="news-table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <div id="news-content" class="news-content">
                <ul>
                  <li>[06/12/2025] Invited talk at <a href="https://ai4cc.net/">CVPR 2025 workshop</a>.</li>
                  <li>[10/28/2025] Invited talk at MIT.</li>
                  <li>[08/16/2025] Awarded <a href="https://risingstars-eecs.mit.edu/participants/yutong-bai-2/">EECS Rising Star in MIT</a>.</li>
                  <li>[09/29/2024] Invited talk at <a href="https://sslwin.org/">ECCV 2024 workshop</a>.</li>
                  <li>[06/18/2024] Invited talk at <a href="https://sites.google.com/view/t4v-cvpr24">CVPR 2024 workshop</a>.</li>
                  <li>[03/08/2024] Invited talk at <a href="https://www.grasp.upenn.edu/events/spring-2024-grasp-seminar-yutong-bai/">Upenn GRASP Seminar</a>.</li>
                  <li>[01/25/2024] Invited talk at Adobe.</li>
                  <li>[01/23/2024] Invited talk at UT Austin.</li>
                  <li>[01/16/2024] Invited talk at Distinguished AI Lecture Series at Imperial College London.</li>
                  <li>[12/06/2024] Invited talk at CMU.</li>
                  <li>[12/03/2024] Invited talk at UCSD.</li>
                  <li>[11/15/2023] Invited talk at UMD.</li>
                  <li>[10/25/2023] Awarded <a href="https://ml.umd.edu/rising-stars-previous-winners">Machine Learning Rising Star in UMD</a>.</li>
                  <li>[09/18/2023] Awarded <a href="https://eecsrisingstars2023.cc.gatech.edu/participants/Yutong_Bai/">EECS Rising Star in Georgia Tech</a>.</li>
                  <li>[06/21/2022] Awarded CVPR Best Paper Finalist.</li>
                </ul>
              </div>
            </td>
          </tr>
          </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Her research aims to build up the AI system with less supervision and strong robustness. Explorations include representation learning, self-supervised learning, and generative modeling.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <div style="font-size:20px;font-weight:bold;display:inline;">Publications</div>
              <div class="pub-filter-menu" style="display:inline; margin-left: 10px;">
                (
                  <span class="pub-filter-link active" data-filter="selected" onclick="filterPublications('selected')">show selected</span> /
                  <span class="pub-filter-link" data-filter="all-date" onclick="filterPublications('all-date')">show all by date</span> /
                  <span class="pub-filter-link" data-filter="all-topic" onclick="filterPublications('all-topic')">show all by topic</span>
                )
              </div>
            </td>
          </tr>
        </tbody></table>

        <table id="publications-table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <video width="100%" controls poster="">
                <source src="images/peva.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="http://arxiv.org/abs/2506.21552">
                  <papertitle>Whole-Body Conditioned Egocentric Video Prediction</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>*, <span class="author-name">Danny Tran</span>*, <span class="author-name">Amir Bar</span>*, <span class="author-name"><a href="http://yann.lecun.com/">Yann LeCun</a></span><span style="font-size: 0.8em;">†</span>, <span class="author-name"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a></span><span style="font-size: 0.8em;">†</span>, <span class="author-name"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span><span style="font-size: 0.8em;">†</span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="peva">
                <a href="http://arxiv.org/abs/2506.21552">paper</a> &nbsp;/&nbsp;
                <a href="https://dannytran123.github.io/PEVA/">project page</a>
              </div>
            </td>
          </tr>
          
          

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='static/images/visual_sentences.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2312.00785">
                  <papertitle>Sequential Modeling Enables Scalable Learning for Large Vision Models</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>*, <span class="author-name">Xinyang Geng</span>*, <span class="author-name">Karttikeya Mangalam</span>, <span class="author-name">Amir Bar</span>, <span class="author-name">Alan Yuille</span>, <span class="author-name"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a></span>, <span class="author-name"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span>, <span class="author-name">Alexei A. Efros</span>
                <br>
                CVPR, 2024
                <br>
              </p>
              <div class="paper" id="lvm">
                <a href="https://arxiv.org/abs/2312.00785">paper</a> &nbsp/&nbsp
                <a href="https://yutongbai.com/lvm.html">project page</a> &nbsp/&nbsp
                <a href="https://github.com/ytongbai/LVM">code</a> &nbsp/&nbsp
                <a href="https://github.com/ytongbai/LVM">model</a>
              </div>

            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bai2022point.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/pdf/2202.04639.pdf">
                  <papertitle>Point-Level Region Contrast for Object Detection Pre-Training</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name"><a href="https://xinleic.xyz/">Xinlei Chen</a></span>, <span class="author-name"><a href="https://alexander-kirillov.github.io/">Alexander Kirillov</a></span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>, <span class="author-name"><a href="https://acberg.com/">Alexander C. Berg</a></span>
                <br>
                CVPR, 2022 &nbsp <font color="red"><strong>(Nominated for CVPR Best Paper - Top 0.4%)</strong></font>
                
                <br>
              </p>
              <div class="paper" id="bai2022point">
                <a href="https://arxiv.org/pdf/2202.04639.pdf">paper</a> &nbsp/&nbsp
                <a href="https://github.com/facebookresearch/PLRC">code</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/173gpMUxv_3blUABBalKYZZ7lgxrhFLa2/view?usp=sharing">video</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1mmnzBO-DtCw46pqLhFn6ok2SYfBjojom/view?usp=sharing">poster</a>
              </div>

            </td>
          </tr>
          

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/tardis.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2506.11302">
                  <papertitle>TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy</papertitle>
                </a>
                <br>
                <span class="author-name">Héctor Carrión</span>*, <span class="author-name yutong-bai">Yutong Bai</span>*, <span class="author-name">Víctor A. Hernández Castro</span>*, <span class="author-name">Kishan Panaganti</span>, <span class="author-name">Ayush Zenith</span>, <span class="author-name">Matthew Trang</span>, <span class="author-name">Tony Zhang</span>, <span class="author-name"><a href="https://www.eas.caltech.edu/people/perona">Pietro Perona</a></span>, <span class="author-name"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="tardis">
                <a href="https://arxiv.org/abs/2506.11302">paper</a> &nbsp/&nbsp
                <a href="https://www.tera-ai.com/blog/tardis">project page</a> &nbsp/&nbsp
                <a href="https://huggingface.co/datasets/Tera-AI/STRIDE">data</a> &nbsp/&nbsp
                <a href="https://huggingface.co/datasets/Tera-AI/STRIDE">code</a> &nbsp/&nbsp
                <a href="https://huggingface.co/datasets/Tera-AI/STRIDE">model</a>
              </div>
            </td>
          </tr>
          

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/mochi.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2409.05862">
                  <papertitle>Evaluating Multiview Object Consistency in Humans and Image Models</papertitle>
                </a>
                <br>
                <span class="author-name">Tyler Bonnen</span>, <span class="author-name">Stephanie Fu</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Thomas O'Connell</span>, <span class="author-name">Yoni Friedman</span>, <span class="author-name">Nancy Kanwisher</span>, <span class="author-name">Josh Tenenbaum</span>, <span class="author-name">Alexei Efros</span>
                <br>
                NeurIPS, 2024
                <br>
              </p>
              <div class="paper" id="mochi">
                <a href="https://arxiv.org/abs/2409.05862">paper</a> &nbsp/&nbsp
                <a href="https://tzler.github.io/MOCHI/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/tzler/mochi_code">code</a> &nbsp/&nbsp
                <a href="https://huggingface.co/datasets/tzler/MOCHI">data</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/diff_prop.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2306.00974">
                  <papertitle>Intriguing Properties of Text-guided Diffusion Models</papertitle>
                </a>
                <br>
                <span class="author-name">Qihao Liu</span>, <span class="author-name">Adam Kortylewski</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Song Bai</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>
                <br>
                ICLR, 2024
                <br>
              </p>
              <div class="paper" id="diff_prop">
                <a href="https://arxiv.org/abs/2306.00974">paper</a> &nbsp;/&nbsp
                <a href="https://sage-diffusion.github.io/">project page</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/vl.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2411.05001">
                  <papertitle>Analyzing The Language of Visual Tokens</papertitle>
                </a>
                <br>
                <span class="author-name">David M Chan</span>, <span class="author-name">Rodolfo Corona</span>, <span class="author-name">Joonyong Park</span>, <span class="author-name">Cheol Jun Cho</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Trevor Darrell</span>
                <br>
                Tech Report, 2024
                <br>
              </p>
              <div class="paper" id="vl">
                <a href="https://arxiv.org/abs/2411.05001">paper</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/kiva.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2407.17773">
                  <papertitle>KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models</papertitle>
                </a>
                <br>
                <span class="author-name">Eunice Yiu</span>, <span class="author-name">Maan Qraitem</span>, <span class="author-name">Anisa Noor Majhi</span>, <span class="author-name">Charlie Wong</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Shiry Ginosar</span>, <span class="author-name">Alison Gopnik</span>, <span class="author-name">Kate Saenko</span>
                <br>
                ICLR, 2025
                <br>
              </p>
              <div class="paper" id="kiva">
                <a href="https://arxiv.org/abs/2407.17773">paper</a> &nbsp/&nbsp
                <a href="https://ey242.github.io/kiva.github.io/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/ey242/KiVA">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/mood.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2504.15145">
                  <papertitle>"I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts</papertitle>
                </a>
                <br>
                <span class="author-name">Huzheng Yang</span>, <span class="author-name">Katherine Xu</span>, <span class="author-name">Michael D. Grossberg</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Jianbo Shi</span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="mood">
                <a href="https://arxiv.org/abs/2504.15145">paper</a> &nbsp/&nbsp
                <a href="https://huzeyann.github.io/mspace/">project page</a> &nbsp/&nbsp
                <a href="https://huggingface.co/spaces/huzey/MoodSpace">demo</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bai2022masked.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2208.12256">
                  <papertitle>Masked Autoencoders Enable Efficient Knowledge Distillers</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Zeyu Wang</span>, <span class="author-name">Junfei Xiao</span>, <span class="author-name">Chen Wei</span>, <span class="author-name">Huiyu Wang</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan L Yuille</a></span>, <span class="author-name">Yuyin Zhou</span>, <span class="author-name">Cihang Xie</span>
                <br>
                CVPR, 2023
                <br>
              </p>
              <div class="paper" id="bai2022masked">
                <a href="https://arxiv.org/abs/2208.12256">paper</a> &nbsp/&nbsp
                <a href="https://github.com/UCSC-VLAA/DMAE">code</a> &nbsp/&nbsp
                <a href="https://github.com/UCSC-VLAA/DMAE">model</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bai2020can.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2011.13046">
                  <papertitle>Can Temporal Information Help with Contrastive Self-Supervised Learning?</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>,
                <span class="author-name">Haoqi Fan</span>,
                <span class="author-name">Ishan Misra</span>,
                <span class="author-name">Ganesh Venkatesh</span>,
                <span class="author-name">Yongyi Lu</span>,
                <span class="author-name">Yuyin Zhou</span>,
                <span class="author-name">Qihang Yu</span>,
                <span class="author-name">Vikas Chandra</span>,
                <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>
                <br>
                Tech Report, 2020
                <br>
              </p>
              <div class="paper" id="bai2020can">
                <a href="https://arxiv.org/abs/2011.13046">paper</a>
              </div>
            </td>
          </tr>


          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
                <img src='images/yu2019c2fnas.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/pdf/1912.09628.pdf">
                  <papertitle>C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation</papertitle>
                </a>
                <br>
                <span class="author-name">Qihang Yu</span>,
                <span class="author-name">Dong Yang</span>,
                <span class="author-name">Holger Roth</span>,
                <span class="author-name yutong-bai">Yutong Bai</span>,
                <span class="author-name">Yixiao Zhang</span>,
                <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>,
                <span class="author-name">Daguang Xu</span>
                <br>
                CVPR, 2020
                <br>
              </p>
              <div class="paper" id="yu2019c2fnas">
                <a href="https://arxiv.org/pdf/1912.09628.pdf">paper</a>
              </div>
            </td>
          </tr>


          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bai2019sp.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/pdf/1811.11823.pdf">
                  <papertitle>Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints from Limited Training Data</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Qing Liu</span>, <span class="author-name">Lingxi Xie</span>, <span class="author-name">Weichao Qiu</span>, <span class="author-name">Yan Zheng</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>
                <br>
                ICCV, 2019
                <br>
              </p>
              <div class="paper" id="bai2019sp">
                <a href="https://arxiv.org/pdf/1811.11823.pdf">paper</a> &nbsp/&nbsp
                <a href="https://github.com/ytongbai/SemanticPartDetection">code</a>
                
              </div>
            </td>
          </tr>


          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/liu2019clevr.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/pdf/1901.00850.pdf">
                  <papertitle>Clevr-ref+: Diagnosing Visual Reasoning with Referring Expressions</papertitle>
                </a>
                <br>
                <span class="author-name">Runtao Liu</span>, <span class="author-name">Chenxi Liu</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan L Yuille</a></span>
                <br>
                CVPR, 2019
                <br>
              </p>
              <div class="paper" id="liu2019clevr">
                <a href="https://arxiv.org/pdf/1901.00850.pdf">paper</a> &nbsp/&nbsp
                <a href="https://www.cs.jhu.edu/~cxliu/2019/clevr-ref+">project page</a> 
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bai2023coke.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/pdf/2009.14115.pdf">
                  <papertitle>CoKe: Contrastive Learning for Robust Keypoint Detection</papertitle>
                </a>
                <br>
                <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Angtian Wang</span>, <span class="author-name">Adam Kortylewski</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>
                <br>
                WACV, 2023
                <br>
              </p>
              <div class="paper" id="bai2023coke">
                <a href="https://arxiv.org/pdf/2009.14115.pdf">paper</a>
              </div>
            </td>
          </tr>


          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/xiao2023mae.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.pdf">
                  <papertitle>Delving Into Masked Autoencoders for Multi-Label Thorax Disease Classification</papertitle>
                </a>
                <br>
                <span class="author-name">Junfei Xiao</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name"><a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a></span>, <span class="author-name">Zongwei Zhou</span>
                <br>
                WACV, 2023
                <br>
              </p>
              <div class="paper" id="xiao2023mae">
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.pdf">paper</a> &nbsp/&nbsp
                <a href="https://github.com/lambert-x/Medical_MAE">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/alphaone.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2505.24863">
                  <papertitle>AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time</papertitle>
                </a>
                <br>
                <span class="author-name">Junyu Zhang</span>, <span class="author-name">Runpei Dong</span>, <span class="author-name">Han Wang</span>, <span class="author-name">Xuying Ning</span>, <span class="author-name">Haoran Geng</span>, <span class="author-name">Peihao Li</span>, <span class="author-name">Xialin He</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Jitendra Malik</span>, <span class="author-name">Saurabh Gupta</span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="alphaone">
                <a href="https://arxiv.org/abs/2505.24863">paper</a> &nbsp/&nbsp
                <a href="https://alphaone-project.github.io/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/ASTRAL-Group/AlphaOne">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/reorder.gif'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2505.23751">
                  <papertitle>REOrdering Patches Improves Vision Models</papertitle>
                </a>
                <br>
                <span class="author-name">Declan Kutscher</span>, <span class="author-name">David M Chan</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Trevor Darrell</span>, <span class="author-name">Ritwik Gupta</span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="reorder">
                <a href="https://arxiv.org/abs/2505.23751">paper</a> &nbsp/&nbsp
                <a href="https://d3tk.github.io/REOrder/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/d3tk/REOrder">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/av.svg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2412.02611">
                  <papertitle>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</papertitle>
                </a>
                <br>
                <span class="author-name">Kaixiong Gong</span>, <span class="author-name">Kaituo Feng</span>, <span class="author-name">Bohao Li</span>, <span class="author-name">Yibing Wang</span>, <span class="author-name">Mofan Cheng</span>, <span class="author-name">Shijia Yang</span>, <span class="author-name">Jiaming Han</span>, <span class="author-name">Benyou Wang</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Zhuoran Yang</span>, <span class="author-name">Xiangyu Yue</span>
                <br>
                Tech Report, 2024
                <br>
              </p>
              <div class="paper" id="avodyssey">
                <a href="https://arxiv.org/abs/2412.02611">paper</a> &nbsp/&nbsp
                <a href="https://av-odyssey.github.io/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/AV-Odyssey/AV-Odyssey">code</a> &nbsp/&nbsp
                <a href="https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench">data</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/task_vector.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2404.05729">
                  <papertitle>Finding Visual Task Vectors</papertitle>
                </a>
                <br>
                <span class="author-name">Alberto Hojel</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Trevor Darrell</span>, <span class="author-name">Amir Globerson</span>, <span class="author-name">Amir Bar</span>
                <br>
                ECCV, 2024
                <br>
              </p>
              <div class="paper" id="visualtaskvectors">
                <a href="https://arxiv.org/abs/2404.05729">paper</a> &nbsp/&nbsp
                <a href="https://github.com/alhojel/visual_task_vectors">code</a> &nbsp/&nbsp
                <a href="https://github.com/alhojel/visual_task_vectors">model</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/mat.png'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Mask_Guided_Matting_via_Progressive_Refinement_Network_CVPR_2021_paper.pdf">
                  <papertitle>Mask Guided Matting via Progressive Refinement Network</papertitle>
                </a>
                <br>
                <span class="author-name">Qihang Yu</span>, <span class="author-name">Jianming Zhang</span>, <span class="author-name">He Zhang</span>, <span class="author-name">Yilin Wang</span>, <span class="author-name">Zhe Lin</span>, <span class="author-name">Ning Xu</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Alan Yuille</span>
                <br>
                CVPR, 2021
                <br>
              </p>
              <div class="paper" id="mgmatting">
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Mask_Guided_Matting_via_Progressive_Refinement_Network_CVPR_2021_paper.pdf">paper</a> &nbsp/&nbsp
                <a href="https://github.com/yucornetto/MGMatting">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/gg.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2106.02277">
                  <papertitle>Glance-and-Gaze Vision Transformer</papertitle>
                </a>
                <br>
                <span class="author-name">Qihang Yu</span>, <span class="author-name">Yingda Xia</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Yongyi Lu</span>, <span class="author-name">Alan L. Yuille</span>, <span class="author-name">Wei Shen</span>
                <br>
                NeurIPS, 2021
                <br>
              </p>
              <div class="paper" id="yu2021glance">
                <a href="https://arxiv.org/abs/2106.02277">paper</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/rob_2.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2206.03452">
                  <papertitle>Can CNNs Be More Robust Than Transformers?</papertitle>
                </a>
                <br>
                <span class="author-name">Zeyu Wang</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Yuyin Zhou</span>, <span class="author-name">Cihang Xie</span>
                <br>
                ICLR, 2023
                <br>
              </p>
              <div class="paper" id="wang2022cnn">
                <a href="https://arxiv.org/abs/2206.03452">paper</a> &nbsp/&nbsp
                <a href="https://github.com/UCSC-VLAA/RobustCNN">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/llarva.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2406.11815">
                  <papertitle>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</papertitle>
                </a>
                <br>
                <span class="author-name">Dantong Niu</span>, <span class="author-name">Yuvan Sharma</span>, <span class="author-name">Giscard Biamby</span>, <span class="author-name">Jerome Quenum</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Baifeng Shi</span>, <span class="author-name">Trevor Darrell</span>, <span class="author-name">Roei Herzig</span>
                <br>
                CoRL, 2024
                <br>
              </p>
              <div class="paper" id="llarva">
                <a href="https://arxiv.org/abs/2406.11815">paper</a> &nbsp/&nbsp
                <a href="https://llarva24.github.io/">project page</a> &nbsp/&nbsp
                <a href="https://github.com/Dantong88/LLARVA">code</a> &nbsp/&nbsp
                <a href="https://github.com/Dantong88/LLARVA/blob/main/docs/DATASET.md">data</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/cold_start.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://proceedings.mlr.press/v227/chen24a/chen24a.pdf">
                  <papertitle>Making Your First Choice: To Address Cold Start Problem in Medical Active Learning</papertitle>
                </a>
                <br>
                <span class="author-name">Liangyu Chen</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Siyu Huang</span>, <span class="author-name">Yongyi Lu</span>, <span class="author-name">Bihan Wen</span>, <span class="author-name">Alan Yuille</span>, <span class="author-name">Zongwei Zhou</span>
                <br>
                PMLR, 2022
                <br>
              </p>
              <div class="paper" id="coldstart">
                <a href="https://proceedings.mlr.press/v227/chen24a/chen24a.pdf">paper</a> &nbsp/&nbsp
                <a href="https://github.com/cliangyu/CSVAL">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/bio.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://www.cell.com/iscience/fulltext/S2589-0042(23)01320-2">
                  <papertitle>Focalizing regions of biomarker relevance facilitates biomarker prediction on histopathological images</papertitle>
                </a>
                <br>
                <span class="author-name">Jiefeng Gan</span>, <span class="author-name">Hanchen Wang</span>, <span class="author-name">Hui Yu</span>, <span class="author-name">Zitong He</span>, <span class="author-name">Wenjuan Zhang</span>, <span class="author-name">Ke Ma</span>, <span class="author-name">Lianghui Zhu</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Zongwei Zhou</span>, <span class="author-name">Alan Yullie</span>, <span class="author-name">Xiang Bai</span>, <span class="author-name">Mingwei Wang</span>, <span class="author-name">Dehua Yang</span>, <span class="author-name">Yanyan Chen</span>, <span class="author-name">Guoan Chen</span>, <span class="author-name">Joan Lasenby</span>, <span class="author-name">Chao Cheng</span>, <span class="author-name">Jia Wu</span>, <span class="author-name">Jianjun Zhang</span>, <span class="author-name">Xinggang Wang</span>, <span class="author-name">Yaobing Chen</span>, <span class="author-name">Guoping Wang</span>, <span class="author-name">Tian Xia</span>
                <br>
                iScience, 2023
                <br>
              </p>
              <div class="paper" id="bio">
                <a href="https://www.cell.com/iscience/fulltext/S2589-0042(23)01320-2">paper</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/3d.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2503.06469">
                  <papertitle>Vector Quantized Feature Fields for Fast 3D Semantic Lifting</papertitle>
                </a>
                <br>
                <span class="author-name">George Tang</span>, <span class="author-name">Aditya Agarwal</span>, <span class="author-name">Weiqiao Han</span>, <span class="author-name">Trevor Darrell</span>, <span class="author-name yutong-bai">Yutong Bai</span>
                <br>
                Tech Report, 2025
                <br>
              </p>
              <div class="paper" id="vq3d">
                <a href="https://arxiv.org/abs/2503.06469">paper</a>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tdimg" style="padding:20px;width:26%;vertical-align:top">
              <img src='images/fastadvprop.jpg'>
            </td>
            <td class="tdcontent" style="padding:20px;width:74%;vertical-align:top">
              <p>
                <a href="https://arxiv.org/abs/2204.09838">
                  <papertitle>Fast AdvProp</papertitle>
                </a>
                <br>
                <span class="author-name">Jieru Mei</span>, <span class="author-name">Yucheng Han</span>, <span class="author-name yutong-bai">Yutong Bai</span>, <span class="author-name">Yixiao Zhang</span>, <span class="author-name">Yingwei Li</span>, <span class="author-name">Xianhang Li</span>, <span class="author-name">Alan Yuille</span>, <span class="author-name">Cihang Xie</span>
                <br>
                ICLR, 2022
                <br>
              </p>
              <div class="paper" id="fastadvprop">
                <a href="https://arxiv.org/abs/2204.09838">paper</a> &nbsp/&nbsp
                <a href="https://github.com/meijieru/fast_advprop">code</a> &nbsp/&nbsp
                <a href="https://github.com/meijieru/fast_advprop">model</a>
              </div>
            </td>
          </tr>

    
        </tbody></table>

      </td>
    </tr>
  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Publications Filter JavaScript -->
<script>
// Publication data with categories and dates
const publications = [
  // 2025
  {
    id: 'peva',
    title: 'Whole-Body Conditioned Egocentric Video Prediction',
    year: 2025,
    venue: 'Tech Report',
    topics: ['video-prediction', 'egocentric', 'generative'],
    selected: true
  },
  {
    id: 'tardis',
    title: 'TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy',
    year: 2025,
    venue: 'Tech Report',
    topics: ['autonomous-driving', 'dataset', 'world-model'],
    selected: true
  },
  {
    id: 'kiva',
    title: 'KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models',
    year: 2025,
    venue: 'ICLR',
    topics: ['visual-analogies', 'multimodal-models', 'evaluation'],
    selected: true
  },
  {
    id: 'mood',
    title: '"I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts',
    year: 2025,
    venue: 'Tech Report',
    topics: ['visual-concepts', 'mood-spaces', 'expression'],
    selected: true
  },
  {
    id: 'alphaone',
    title: 'AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time',
    year: 2025,
    venue: 'Tech Report',
    topics: ['reasoning-models', 'thinking-slow-fast', 'test-time'],
    selected: false
  },
  {
    id: 'reorder',
    title: 'REOrdering Patches Improves Vision Models',
    year: 2025,
    venue: 'Tech Report',
    topics: ['vision-models', 'patches', 'improvement'],
    selected: false
  },
  // 2024
  {
    id: 'lvm',
    title: 'Sequential Modeling Enables Scalable Learning for Large Vision Models',
    year: 2024,
    venue: 'CVPR',
    topics: ['large-vision-models', 'sequential-modeling', 'scalable-learning'],
    selected: true
  },
  {
    id: 'mochi',
    title: 'Evaluating Multiview Object Consistency in Humans and Image Models',
    year: 2024,
    venue: 'NeurIPS',
    topics: ['multiview-consistency', 'human-vision', 'image-models'],
    selected: true
  },
  {
    id: 'diff_prop',
    title: 'Intriguing Properties of Text-guided Diffusion Models',
    year: 2024,
    venue: 'ICLR',
    topics: ['diffusion-models', 'failure-modes', 'adversarial-search'],
    selected: false
  },
  {
    id: 'vl',
    title: 'Analyzing The Language of Visual Tokens',
    year: 2024,
    venue: 'Tech Report',
    topics: ['visual-tokens', 'language-analysis'],
    selected: false
  },
  {
    id: 'avodyssey',
    title: 'AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?',
    year: 2024,
    venue: 'Tech Report',
    topics: ['multimodal-llms', 'audio-visual-information', 'benchmark'],
    selected: false
  },
  {
    id: 'visualtaskvectors',
    title: 'Finding Visual Task Vectors',
    year: 2024,
    venue: 'ECCV',
    topics: ['visual-task-vectors', 'visual-prompting'],
    selected: false
  },
  {
    id: 'llarva',
    title: 'LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning',
    year: 2024,
    venue: 'CoRL',
    topics: ['robot-learning', 'vision-action', 'instruction-tuning'],
    selected: false
  },
  // 2023
  {
    id: 'bai2022masked',
    title: 'Masked Autoencoders Enable Efficient Knowledge Distillers',
    year: 2023,
    venue: 'CVPR',
    topics: ['masked-autoencoders', 'knowledge-distillation', 'efficient-learning'],
    selected: true
  },
  {
    id: 'wang2022cnn',
    title: 'Can CNNs Be More Robust Than Transformers?',
    year: 2023,
    venue: 'ICLR',
    topics: ['robustness', 'cnn', 'transformer', 'comparison'],
    selected: false
  },
  {
    id: 'bai2023coke',
    title: 'CoKe: Contrastive Learning for Robust Keypoint Detection',
    year: 2023,
    venue: 'WACV',
    topics: ['keypoint-detection', 'contrastive-learning', 'robustness'],
    selected: false
  },
  {
    id: 'xiao2023mae',
    title: 'Delving Into Masked Autoencoders for Multi-Label Thorax Disease Classification',
    year: 2023,
    venue: 'WACV',
    topics: ['masked-autoencoders', 'medical-imaging', 'multi-label-classification'],
    selected: false
  },
  // 2022
  {
    id: 'bai2022point',
    title: 'Point-Level Region Contrast for Object Detection Pre-Training',
    year: 2022,
    venue: 'CVPR',
    topics: ['object-detection', 'contrastive-learning', 'pre-training'],
    selected: true
  },
  {
    id: 'mei2022fastadvprop',
    title: 'Fast AdvProp',
    year: 2022,
    venue: 'ICLR',
    topics: ['adversarial-training', 'efficiency'],
    selected: false
  },
  {
    id: '2022hetrans',
    title: 'TransFG: A Transformer Architecture for Fine-grained Recognition',
    year: 2022,
    venue: 'AAAI',
    topics: ['transformer', 'fine-grained-recognition'],
    selected: false
  },
  // 2021
  {
    id: 'bai2020vitsVScnns',
    title: 'Are Transformers More Robust than CNNs?',
    year: 2021,
    venue: 'NeurIPS',
    topics: ['robustness', 'transformer', 'cnn', 'comparison'],
    selected: true
  },
  {
    id: 'yu2021glance',
    title: 'Glance-and-Gaze Vision Transformer',
    year: 2021,
    venue: 'NeurIPS',
    topics: ['vision-transformer', 'attention-mechanism'],
    selected: false
  },
  {
    id: 'mgmatting',
    title: 'Mask Guided Matting via Progressive Refinement Network',
    year: 2021,
    venue: 'CVPR',
    topics: ['image-matting', 'progressive-refinement'],
    selected: false
  },
  // 2020
  {
    id: 'bai2020can',
    title: 'Can Temporal Information Help with Contrastive Self-Supervised Learning?',
    year: 2020,
    venue: 'Tech report',
    topics: ['contrastive-learning', 'temporal-information', 'self-supervised'],
    selected: true
  },
  {
    id: 'yu2019c2fnas',
    title: 'C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation',
    year: 2020,
    venue: 'CVPR',
    topics: ['neural-architecture-search', 'medical-imaging', '3d-segmentation'],
    selected: false
  },
  // 2019
  {
    id: 'bai2019sp',
    title: 'Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints from Limited Training Data',
    year: 2019,
    venue: 'ICCV',
    topics: ['semantic-part-detection', 'viewpoint-generalization', 'few-shot-learning'],
    selected: true
  },
  {
    id: 'liu2019clevr',
    title: 'Clevr-ref+: Diagnosing Visual Reasoning with Referring Expressions',
    year: 2019,
    venue: 'CVPR',
    topics: ['visual-reasoning', 'referring-expressions', 'diagnostics'],
    selected: true
  },
  {
    id: 'coldstart',
    title: 'Making Your First Choice: To Address Cold Start Problem in Medical Active Learning',
    year: 2022,
    venue: 'PMLR',
    topics: ['active-learning', 'cold-start', 'medical'],
    selected: false
  },
  {
    id: 'bio',
    title: 'Focalizing regions of biomarker relevance facilitates biomarker prediction on histopathological images',
    year: 2023,
    venue: 'iScience',
    topics: ['biomarker', 'histopathology', 'prediction'],
    selected: false
  },
  {
    id: 'vq3d',
    title: 'Vector Quantized Feature Fields for Fast 3D Semantic Lifting',
    year: 2025,
    venue: 'Tech Report',
    topics: ['3d', 'semantic-lifting', 'vector-quantization'],
    selected: false
  },
  {
    id: 'fastadvprop',
    title: 'Fast AdvProp',
    year: 2022,
    venue: 'ICLR',
    topics: ['adversarial-training', 'fast-advprop'],
    selected: false
  }
];

function filterPublications(filterType) {
  // Update button states
  document.querySelectorAll('.pub-filter-link').forEach(btn => btn.classList.remove('active'));
  document.querySelector(`.pub-filter-link[data-filter="${filterType}"]`).classList.add('active');
  
  // Get all publication rows
  const publicationRows = document.querySelectorAll('#publications-table tr');
  
  publicationRows.forEach(row => {
    // Skip header rows and non-publication rows
    if (!row.querySelector('.tdimg') || !row.querySelector('.tdcontent')) {
      return;
    }
    
    // Find the publication ID from the paper div
    const paperDiv = row.querySelector('.paper');
    if (!paperDiv) return;
    
    const publicationId = paperDiv.id;
    const publication = publications.find(p => p.id === publicationId);
    
    // SAFETY: If publication not found, always show the row
    if (!publication) {
      row.classList.remove('hidden');
      return;
    }
    
    let shouldShow = false;
    
    switch(filterType) {
      case 'selected':
        shouldShow = publication.selected;
        break;
      case 'all-date':
        shouldShow = true;
        break;
      case 'all-topic':
        shouldShow = true;
        break;
    }
    
    if (shouldShow) {
      row.classList.remove('hidden');
    } else {
      row.classList.add('hidden');
    }
  });
  
  // Sort by date for all-date, by topic for all-topic, restore original order for selected
  if (filterType === 'all-date') {
    sortPublicationsByDate();
  } else if (filterType === 'all-topic') {
    sortPublicationsByTopic();
  } else if (filterType === 'selected') {
    restoreOriginalOrder();
  }
}

function sortPublicationsByDate() {
  const publicationTable = document.getElementById('publications-table');
  const rows = Array.from(publicationTable.querySelectorAll('tr'));
  
  // Find publication rows
  const publicationRows = rows.filter(row => {
    const paperDiv = row.querySelector('.paper');
    return paperDiv && paperDiv.id;
  });
  
  // Sort by year (descending)
  publicationRows.sort((a, b) => {
    const aId = a.querySelector('.paper').id;
    const bId = b.querySelector('.paper').id;
    const aPub = publications.find(p => p.id === aId);
    const bPub = publications.find(p => p.id === bId);
    return bPub.year - aPub.year;
  });
  
  // Reorder in DOM
  publicationRows.forEach(row => {
    publicationTable.appendChild(row);
  });
}

function sortPublicationsByTopic() {
  const publicationTable = document.getElementById('publications-table');
  const rows = Array.from(publicationTable.querySelectorAll('tr'));
  
  // Find publication rows
  const publicationRows = rows.filter(row => {
    const paperDiv = row.querySelector('.paper');
    return paperDiv && paperDiv.id;
  });
  
  // Sort by first topic alphabetically
  publicationRows.sort((a, b) => {
    const aId = a.querySelector('.paper').id;
    const bId = b.querySelector('.paper').id;
    const aPub = publications.find(p => p.id === aId);
    const bPub = publications.find(p => p.id === bId);
    return aPub.topics[0].localeCompare(bPub.topics[0]);
  });
  
  // Reorder in DOM
  publicationRows.forEach(row => {
    publicationTable.appendChild(row);
  });
}

function restoreOriginalOrder() {
  const publicationTable = document.getElementById('publications-table');
  
  // Remove all current publication rows
  const currentRows = Array.from(publicationTable.querySelectorAll('tr'));
  currentRows.forEach(row => {
    const paperDiv = row.querySelector('.paper');
    if (paperDiv && paperDiv.id) {
      row.remove();
    }
  });
  
  // Add back in original order
  originalOrder.forEach(row => {
    publicationTable.appendChild(row);
  });
}

// Store original order of publications
let originalOrder = [];

// Initialize with selected publications
document.addEventListener('DOMContentLoaded', function() {
  // Save original order
  const publicationTable = document.getElementById('publications-table');
  const rows = Array.from(publicationTable.querySelectorAll('tr'));
  originalOrder = rows.filter(row => {
    const paperDiv = row.querySelector('.paper');
    return paperDiv && paperDiv.id;
  });
  
  filterPublications('selected');
});

// News toggle function
function toggleNews() {
  const newsTable = document.getElementById('news-table');
  const toggle = document.getElementById('news-toggle');
  
  if (newsTable.classList.contains('hidden')) {
    newsTable.classList.remove('hidden');
    toggle.textContent = '▼ Collapse';
  } else {
    newsTable.classList.add('hidden');
    toggle.textContent = '▼ Expand';
  }
}
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

<!-- Hidden Visitor Map - Only visible with special URL parameter -->
<script>
  // Check if URL has special parameter to show visitor map
  const urlParams = new URLSearchParams(window.location.search);
  const showMap = urlParams.get('showmap') === 'true';
  
  if (showMap) {
    // Create visitor map container
    const mapContainer = document.createElement('div');
    mapContainer.style.cssText = `
      position: fixed;
      bottom: 10px;
      right: 10px;
      z-index: 1000;
      background: white;
      border: 1px solid #ccc;
      border-radius: 5px;
      padding: 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      max-width: 300px;
    `;
    
    mapContainer.innerHTML = '<div style="font-size: 12px; margin-bottom: 5px; color: #666;">Visitor Map (Hidden)</div>';
    
    // Add script separately to avoid issues
    const script = document.createElement('script');
    script.type = 'text/javascript';
    script.id = 'clustrmaps';
    script.src = '//clustrmaps.com/map_v2.js?d=oo5Rltc3RHq2JxoymwZoPl0PmfpHDKx0BkgdsyfuxX8&cl=ffffff&w=280';
    mapContainer.appendChild(script);
    
    document.body.appendChild(mapContainer);
  }
</script>

<!-- Alternative: Very subtle visitor map (1x1 pixel) -->
<div style="position: fixed; bottom: 0; right: 0; width: 1px; height: 1px; overflow: hidden; opacity: 0.01;">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=oo5Rltc3RHq2JxoymwZoPl0PmfpHDKx0BkgdsyfuxX8&cl=ffffff&w=1"></script>
</div>

</body>

</html>
